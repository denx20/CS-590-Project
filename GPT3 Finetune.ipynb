{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9383d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from mcts_main import grid_search\n",
    "from sequence_generator import make_possible_terms\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cb785b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test/3/3.csv', names=['prompt', 'completion'], delimiter='],', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e0e6c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 2, -53, 22, -63, -98, 77</td>\n",
       "      <td>[False, False, True, False, True, False, True,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 2, 1, 17, 5, -52, 848</td>\n",
       "      <td>[True, False, False, False, False, False, True...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3, 3, 1, -1, 7, -3, -17</td>\n",
       "      <td>[False, False, False, False, True, False, True...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 3, 33, 75, 51, 21, 225</td>\n",
       "      <td>[False, True, False, True, False, False, True,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 2, -1, -2, -3, -4, -5</td>\n",
       "      <td>[True, True, False, False, False, False, False...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         prompt  \\\n",
       "0  [2, 2, -53, 22, -63, -98, 77   \n",
       "1     [3, 2, 1, 17, 5, -52, 848   \n",
       "2      [3, 3, 1, -1, 7, -3, -17   \n",
       "3    [3, 3, 33, 75, 51, 21, 225   \n",
       "4     [2, 2, -1, -2, -3, -4, -5   \n",
       "\n",
       "                                          completion  \n",
       "0  [False, False, True, False, True, False, True,...  \n",
       "1  [True, False, False, False, False, False, True...  \n",
       "2  [False, False, False, False, True, False, True...  \n",
       "3  [False, True, False, True, False, False, True,...  \n",
       "4  [True, True, False, False, False, False, False...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0c44e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].str.slice(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ee640ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['completion'] = df['completion'].str.slice(1, -1) + ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d5019b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"data/test/3/3.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ecff268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 800 prompt-completion pairs\n",
      "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
      "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
      "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "- [Recommended] Would you like to split into training and validation set? [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified files to `data/train/3/3_prepared_train.jsonl` and `data/train/3/3_prepared_valid.jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"data/train/3/3_prepared_train.jsonl\" -v \"data/train/3/3_prepared_valid.jsonl\" --compute_classification_metrics --classification_n_classes 63\n",
      "\n",
      "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"e <EOS>\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 21.53 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f data/train/3/3.jsonl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62dac6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found potentially duplicated files with name '2_prepared_train.jsonl', purpose 'fine-tune' and size 89749 bytes\n",
      "file-D8NwYn4Q83Md5tcTV7ypoUrL\n",
      "file-QvBbTiiQnKfYKgiQWblzvwib\n",
      "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"data/train/3/3_prepared_train.jsonl\" -v \"data/train/3/3_prepared_valid.jsonl\" -m davinci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "201fbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nterms=2, davinci, 4 epochs, davinci:ft-personal-2022-11-29-06-40-46\n",
    "# nterms=2, curie, 4 epochs, curie:ft-personal-2022-11-30-08-22-34\n",
    "# nterms=2, babbage, 4 epochs, babbage:ft-personal-2022-11-30-08-32-24\n",
    "# nterms=2, ada, 4 epochs, ada:ft-personal-2022-11-30-08-42-32\n",
    "\n",
    "# nterms=3, davinci, 4 epochs, davinci:ft-personal-2022-11-30-08-52-38\n",
    "# nterms=3, curie, 4 epochs, curie:ft-personal-2022-11-30-09-15-04\n",
    "# nterms=3, babbage, 4 epochs, babbage:ft-personal-2022-11-30-09-02-00\n",
    "# nterms=3, ada, 4 epochs, ada:ft-personal-2022-11-30-09-12-47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4125f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gpt3(ft_model=None, nterms=2, eval_train=False):\n",
    "    if not ft_model:\n",
    "        raise ValueError('Must provide a fine-tuned model')\n",
    "    test_dir = 'data/test/'\n",
    "    test_file = test_dir + f'{nterms}/{nterms}_prepared.jsonl'\n",
    "    test_data = pd.read_json(test_file, orient='records', lines=True)['prompt'].tolist()\n",
    "    terms = np.array(make_possible_terms())\n",
    "    n = len(test_data)\n",
    "    rmses = []\n",
    "    correct_cnt = 0\n",
    "    for i in range(0, n, 20):\n",
    "        curr_test = test_data[i:] if n-i < 20 else test_data[i:i+20]\n",
    "        preds = openai.Completion.create(model=ft_model, prompt=curr_test, stop=[' <EOS>'], max_tokens=30, temperature=0)['choices']\n",
    "        for j, obj in enumerate(preds):\n",
    "            pred = obj['text']\n",
    "            pred_mask = np.array(pred.strip().split(', '))=='True'\n",
    "            pred_terms = terms[pred_mask]\n",
    "            seq_list = np.array(test_data[i+j][:-3].split(', ')).astype(int)\n",
    "            rmse = grid_search(seq_list, pred_terms)\n",
    "            rmses.append(rmse)\n",
    "            if rmse == 0:\n",
    "                correct_cnt += 1\n",
    "    print('Mean RMSE on test data:', np.mean(rmses))\n",
    "    print('Percentage of examples solved perfectly:', correct_cnt/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c86c7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE on test data: 90.38050943280014\n",
      "Percentage of examples solved perfectly: 0.075\n"
     ]
    }
   ],
   "source": [
    "evaluate_gpt3('davinci:ft-personal-2022-11-29-06-40-46', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "64f18652e27e01453f7c7c1ea4d267146bfea1df681f47d9eb14fc069da0a720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
